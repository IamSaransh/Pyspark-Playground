{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/17 12:33:24 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/01/17 12:33:25 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "23/01/17 12:33:25 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"logAnalysis\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'SparkSession' object has no attribute 'readstream'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m lines \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadstream\u001b[49m\u001b[38;5;241m.\u001b[39mtext(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m lines\u001b[38;5;241m.\u001b[39mshow(\u001b[38;5;241m2\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'SparkSession' object has no attribute 'readstream'"
     ]
    }
   ],
   "source": [
    "lines = spark.readstream.text(\"data\")\n",
    "lines.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(value='17/06/09 20:10:40 INFO executor.CoarseGrainedExecutorBackend: Registered signal handlers for [TERM, HUP, INT]')]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regex = \"^[0-9]{1,2}\\\\/[0-9]{1,2}\\\\/[0-9]{2} [0-9]{1,2}\\:[0-9]{1,2}\\:[0-9]{2} [A-Z]{4,5} \"\n",
    "lines = lines.filter(lines[\"value\"].rlike(regex))\n",
    "lines.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------+\n",
      "|level|           className|    date|\n",
      "+-----+--------------------+--------+\n",
      "| INFO|executor.CoarseGr...|17/06/09|\n",
      "| INFO|spark.SecurityMan...|17/06/09|\n",
      "+-----+--------------------+--------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import split, explode\n",
    "df_with_log_and_class = lines.withColumn(\"level\", split(lines['value'], ' ').getItem(2)) \\\n",
    "                        .withColumn('className', split(lines[\"value\"], \" \").getItem(3)) \\\n",
    "                        .withColumn('date', split(lines[\"value\"], \" \").getItem(0)) \\\n",
    "                        .drop('value')\n",
    "df_with_log_and_class.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- level: string (nullable = true)\n",
      " |-- className: string (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_with_log_and_class.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+------------+\n",
      "|level|           className|count(level)|\n",
      "+-----+--------------------+------------+\n",
      "|ERROR|  slf4j.Slf4jLogger:|           1|\n",
      "|ERROR|storage.DiskBlock...|           1|\n",
      "| INFO|  slf4j.Slf4jLogger:|           1|\n",
      "|ERROR|netty.NettyBlockT...|           1|\n",
      "| INFO|storage.DiskBlock...|           1|\n",
      "| INFO|netty.NettyBlockT...|           1|\n",
      "|ERROR|storage.BlockMana...|           2|\n",
      "| INFO|storage.BlockMana...|           2|\n",
      "|ERROR|           Remoting:|           2|\n",
      "| INFO|           Remoting:|           2|\n",
      "| INFO|         util.Utils:|           2|\n",
      "|ERROR|         util.Utils:|           2|\n",
      "| INFO|Configuration.dep...|           5|\n",
      "|ERROR|Configuration.dep...|           5|\n",
      "| INFO|spark.SecurityMan...|           6|\n",
      "|ERROR|spark.SecurityMan...|           6|\n",
      "| INFO|mapred.SparkHadoo...|          30|\n",
      "|ERROR|mapred.SparkHadoo...|          30|\n",
      "|ERROR|      rdd.HadoopRDD:|          45|\n",
      "| INFO|      rdd.HadoopRDD:|          45|\n",
      "| INFO|output.FileOutput...|          60|\n",
      "|ERROR|output.FileOutput...|          60|\n",
      "|ERROR|broadcast.Torrent...|          74|\n",
      "| INFO|broadcast.Torrent...|          74|\n",
      "|ERROR| spark.CacheManager:|          75|\n",
      "| INFO| spark.CacheManager:|          75|\n",
      "|ERROR|storage.MemoryStore:|         150|\n",
      "| INFO|storage.MemoryStore:|         150|\n",
      "|ERROR|storage.BlockMana...|         257|\n",
      "| INFO|storage.BlockMana...|         257|\n",
      "| INFO|executor.CoarseGr...|         308|\n",
      "|ERROR|executor.CoarseGr...|         308|\n",
      "| INFO|python.PythonRunner:|         375|\n",
      "|ERROR|python.PythonRunner:|         375|\n",
      "| INFO|  executor.Executor:|         606|\n",
      "|ERROR|  executor.Executor:|         606|\n",
      "+-----+--------------------+------------+\n",
      "\n",
      "23/01/17 12:24:36 WARN HeartbeatReceiver: Removing executor driver with no recent heartbeats: 345385 ms exceeds timeout 120000 ms\n",
      "23/01/17 12:24:36 WARN SparkContext: Killing executors is not supported by current scheduler.\n"
     ]
    }
   ],
   "source": [
    "df_with_log_and_class.createOrReplaceTempView(\"df1_view\")\n",
    "spark.sql(\"select level,className,count(level) from df1_view   group by className, level order by count(level) \").show(10000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
